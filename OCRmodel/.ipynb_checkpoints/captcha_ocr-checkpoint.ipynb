{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckjBXn5IeSrj"
   },
   "source": [
    "# OCR model for reading Captchas\n",
    "\n",
    "**Author:** [A_K_Nain](https://twitter.com/A_K_Nain)<br>\n",
    "**Date created:** 2020/06/14<br>\n",
    "**Last modified:** 2020/06/26<br>\n",
    "**Description:** How to implement an OCR model using CNNs, RNNs and CTC loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fso7vl0VeSrl"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates a simple OCR model built with the Functional API. Apart from\n",
    "combining CNN and RNN, it also illustrates how you can instantiate a new layer\n",
    "and use it as an \"Endpoint layer\" for implementing CTC loss. For a detailed\n",
    "guide to layer subclassing, please check out\n",
    "[this page](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)\n",
    "in the developer guides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJYf6ualeSrm"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LVoWvj8keSrq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6DFgI78eSrr"
   },
   "source": [
    "## Load the data: [Captcha Images](https://www.kaggle.com/fournierp/captcha-version-2-images)\n",
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rbh1r_wNeSrr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   159  100   159    0     0    159      0  0:00:01 --:--:--  0:00:01   636\n",
      "\n",
      " 23 8863k   23 2108k    0     0  2108k      0  0:00:04  0:00:01  0:00:03 2108k\n",
      " 43 8863k   43 3820k    0     0  1910k      0  0:00:04  0:00:02  0:00:02 1712k\n",
      " 69 8863k   69 6124k    0     0  2041k      0  0:00:04  0:00:03  0:00:01 2008k\n",
      " 93 8863k   93 8316k    0     0  2079k      0  0:00:04  0:00:04 --:--:-- 2069k\n",
      "100 8863k  100 8863k    0     0  2215k      0  0:00:04  0:00:04 --:--:-- 2029k\n",
      "'unzip' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exâ€šcutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\n",
    "!unzip -qq captcha_images_v2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6Oq5E0yeSrs"
   },
   "source": [
    "The dataset contains 1040 captcha files as `png` images. The label for each sample is a string,\n",
    "the name of the file (minus the file extension).\n",
    "We will map each character in the string to an integer for training the model. Similary,\n",
    "we will need to map the predictions of the model back to strings. For this purpose\n",
    "we will maintain two dictionaries, mapping characters to integers, and integers to characters,\n",
    "respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5rHLndHNeSrs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  4015\n",
      "Number of labels found:  4015\n",
      "Number of unique characters:  16\n",
      "Characters present:  {'e', '1', '0', '4', 'l', 't', '5', '7', '9', 's', '3', '8', '2', 'a', '6', '-'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the data directory\n",
    "data_dir = Path(\"./captcha_images_v2/\")\n",
    "# Get list of all the images\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "\n",
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n",
    "\n",
    "# Batch size for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "# Desired image dimensions\n",
    "img_width = 200\n",
    "img_height = 50\n",
    "\n",
    "# Factor by which the image is going to be downsampled\n",
    "# by the convolutional blocks. We will be using two\n",
    "# convolution blocks and each block will have\n",
    "# a pooling layer which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "max_length = max([len(label) for label in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYNzeZLYeSrt"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9R8_WzWseSrt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": img, \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zp0UuXJeSru"
   },
   "source": [
    "## Create `Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rmFOjq-deSrv"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bbzbslheSrv"
   },
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gn7LiM1MeSrw"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [8], [batch]: [7] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1080/2892370103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6896\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6897\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6898\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [8], [batch]: [7] [Op:IteratorGetNext]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sdy-_qZmeSrw"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-_0oAyfeSrw"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR9XYmrseSrx"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XR_G3GXTeSrx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [7], [batch]: [8]\n\t [[node IteratorGetNext (defined at C:\\Users\\Shidono\\AppData\\Local\\Temp/ipykernel_1080/2877853976.py:9) ]]\n  (1) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [7], [batch]: [8]\n\t [[node IteratorGetNext (defined at C:\\Users\\Shidono\\AppData\\Local\\Temp/ipykernel_1080/2877853976.py:9) ]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_14507]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1080/2877853976.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [7], [batch]: [8]\n\t [[node IteratorGetNext (defined at C:\\Users\\Shidono\\AppData\\Local\\Temp/ipykernel_1080/2877853976.py:9) ]]\n  (1) Invalid argument:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [7], [batch]: [8]\n\t [[node IteratorGetNext (defined at C:\\Users\\Shidono\\AppData\\Local\\Temp/ipykernel_1080/2877853976.py:9) ]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_14507]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwufZieCeSrx"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtblAOeReSry"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the prediction model by extracting layers till the output layer\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "prediction_model.summary()\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "#  Let's check results on some validation samples\n",
    "for batch in validation_dataset.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    orig_texts = []\n",
    "    for label in batch_labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n",
    "    for i in range(len(pred_texts)):\n",
    "        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n",
    "        img = img.T\n",
    "        title = f\"Prediction: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "captcha_ocr",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
